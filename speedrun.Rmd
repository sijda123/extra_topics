---
title: "P8105 Speedrun"
output: github_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

How fast can we get through P8105 with what we know + ChatGPT?

```{r}
library(tidyverse)
```

I'm going to give ChatGPT (5.1 instant) this prompt:

> I'm a biostatistician working in R, and I prefer to use tidyverse functions in scripts / RMD files in an R project; all data files are in a subdirectory called "speedrun_data". I'm going to give a variety of coding-related questions.

Next we'll revisit some classic examples and see how we do!

## FAS Litters and Pups

```{r}
library(tidyverse)

#------------------------------------------------------------
# 1. Import
#------------------------------------------------------------
litters_raw <- read_csv("speedrun_data/FAS_litters.csv")
pups_raw    <- read_csv("speedrun_data/FAS_pups.csv", skip = 3, na = c("", ".", "NA"))

#------------------------------------------------------------
# 2. Clean column names
#    - Convert to snake_case
#    - Standardize litter identifiers
#------------------------------------------------------------
litters <- 
	litters_raw %>%
  janitor::clean_names() %>%
	rename(litter_id = litter_number) |> 
  mutate(
    litter_id = str_to_lower(litter_id),
    litter_id = str_replace_all(litter_id, "\\s+", "")
  )

pups <- pups_raw %>%
  janitor::clean_names() %>%
	rename(litter_id = litter_number) |> 
  mutate(
    litter_id = str_to_lower(litter_id),
    litter_id = str_replace_all(litter_id, "\\s+", "")
  )

#------------------------------------------------------------
# 3. Tidy data
#    (examples; adjust depending on your variables)
#------------------------------------------------------------

# Litters dataset:
# - Convert dates to Date class
# - Convert categorical variables to factors
litters <- litters %>%
  mutate(
    gd0 = as.Date(gd0),
    gd18 = as.Date(gd18),
    group = factor(group),
    dose = factor(dose, ordered = TRUE)
  )

# Pups dataset:
# - Convert sex to factor
# - Ensure numeric weights
pups <- pups %>%
  mutate(
    sex = factor(sex),
    pd = as.numeric(pd)
  )

#------------------------------------------------------------
# 4. Merge: join pups to litters by litter_id
#------------------------------------------------------------
fas_merged <- pups %>%
  left_join(litters, by = "litter_id")

#------------------------------------------------------------
# 5. Inspect final cleaned data
#------------------------------------------------------------
glimpse(fas_merged)
```



## Weather Data + EDA

Import then use ChatGPT code.

```{r}
weather = 
	read_csv("speedrun_data/weather_df.csv") |> 
	rename(station = name)
```

```{r}
weather %>%
  group_by(station) %>%
  summarize(
    n_days = n(),
    mean_tmax = mean(tmax, na.rm = TRUE),
    mean_tmin = mean(tmin, na.rm = TRUE),
    mean_prcp = mean(prcp, na.rm = TRUE)
  )
```

max temp plot

```{r}
ggplot(weather, aes(date, tmax, color = station)) +
  geom_line(alpha = 0.7) +
  labs(title = "Daily Maximum Temperature by Station",
       y = "Max Temperature",
       x = "Date") +
  theme_minimal()
```

temperature distribution


```{r}
weather %>%
  pivot_longer(cols = c(tmax, tmin),
               names_to = "variable",
               values_to = "value") %>%
  ggplot(aes(value, fill = station)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ variable, scales = "free") +
  labs(title = "Distribution of Temperatures",
       x = "Temperature") +
  theme_minimal()
```

seasonal trends

```{r}
weather_monthly <- weather %>%
  mutate(month = floor_date(date, "month")) %>%
  group_by(station, month) %>%
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE),
    mean_tmin = mean(tmin, na.rm = TRUE),
    .groups = "drop"
  )

# Plot monthly mean tmax
ggplot(weather_monthly, aes(month, mean_tmax, color = station)) +
  geom_line(size = 1.1) +
  labs(title = "Monthly Mean Tmax by Station",
       y = "Mean Tmax",
       x = "Month") +
  theme_minimal()
```


## AirBNB Leaflet map

```{r}
library(p8105.datasets)
library(leaflet)

data("nyc_airbnb")

nyc_airbnb =
	nyc_airbnb |> 
	mutate(
		rating = review_scores_location/2
	) |> 
	filter(price < 1000)
```

Create a leaflet map

```{r}
# Color scale based on price (continuous)
price_pal <- colorNumeric(
  palette = "viridis",   # perceptually uniform, colorblind-friendly
  domain  = nyc_airbnb$price
)

leaflet(data = nyc_airbnb) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%   # greyscale/light map
  addCircleMarkers(
    ~lat, ~long,
    radius = 4,
    stroke = FALSE,
    fillOpacity = 0.7,
    fillColor = ~price_pal(price),
    popup = ~paste0(
      "<b>Price: </b>$", price, "<br>",
      "<b>Rating: </b>", rating, "<br>",
      "<b>Lat: </b>", lat, "<br>",
      "<b>Long: </b>", long
    )
  ) %>%
  addLegend(
    "bottomright",
    pal = price_pal,
    values = ~price,
    title = "Price",
    opacity = 1
  )

```


## Chloropleth?




## Midterm clean and EDA

```{r}
library(tidyverse)
library(readxl)
library(janitor)
library(lubridate)

# Import, skipping first 6 codebook rows
mtp_raw <- read_excel(
  "speedrun_data/p8105_mtp_data.xlsx",
  skip = 6
)

# Clean names + inspect
mtp <- mtp_raw %>%
  janitor::clean_names()

glimpse(mtp)
summary(mtp)
```

clean

```{r}
mtp <- mtp %>%
  mutate(
    # Convert categorical variables
    sex = case_when(
      sex %in% c("M", "Male", 1) ~ "male",
      sex %in% c("F", "Female", 2) ~ "female",
      TRUE ~ NA_character_
    ),
    sex = factor(sex),

    # Ensure numeric variables are numeric
    age = as.numeric(age),
    posture_score = as.numeric(posture_score),
    protuberance_size = as.numeric(protuberance_size),

    # Convert dates if present
    date = suppressWarnings(ymd(date))
  )
```



## Birthday problem

Look at birthday problem

Function

```{r}
birthday_sim <- function(n) {
  # Simulate n birthdays (1â€“365)
  bdays <- sample(1:365, size = n, replace = TRUE)
  
  # Return TRUE if at least one match
  any(duplicated(bdays))
}
```

Example: 23 people

```{r}
set.seed(123)

n_sims <- 10000
n <- 23

results <- tibble(
  sim = 1:n_sims,
  match = map_lgl(sim, ~ birthday_sim(n))
)

mean(results$match)
```


Do this a lot

```{r}
set.seed(123)

group_sizes <- 2:60
n_sims <- 5000

birthday_results <- 
  crossing(
    group_size = group_sizes,
    sim = 1:n_sims
  ) %>%
  mutate(
    match = map_lgl(group_size, \(n) birthday_sim(n))
  ) %>%
  group_by(group_size) %>%
  summarize(prob_match = mean(match), .groups = "drop")
```


```{r}
library(ggplot2)

birthday_results %>%
  ggplot(aes(group_size, prob_match)) +
  geom_line(size = 1, color = "steelblue") +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  labs(
    title = "Birthday Problem Simulation",
    subtitle = "Probability that at least two people share a birthday",
    x = "Group size",
    y = "Probability"
  ) +
  theme_minimal(base_size = 14)
```

